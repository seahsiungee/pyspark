{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62795b5-bd8d-4804-96cb-f4bbb9fa8f28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed successfully.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp, max, lit, trim\n",
    "from datetime import datetime\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Elasticsearch to Delta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configuration\n",
    "es_cloud_url = dbutils.secrets.get(scope=\"snocko\", key=\"es_cloud_url\")\n",
    "es_user = dbutils.secrets.get(scope=\"snocko\", key=\"es_user\")\n",
    "es_pass = dbutils.secrets.get(scope=\"snocko\", key=\"es_pass\")\n",
    "\n",
    "delta_base_path = \"hive_metastore.default.\"\n",
    "tracking_table_name = \"hive_metastore.default.tracking_table\"\n",
    "\n",
    "# Define schema\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, LongType, TimestampType, ArrayType, FloatType, MapType\n",
    "\n",
    "trade_txn_schema = StructType([\n",
    "    StructField(\"create_timestamp\", TimestampType()),\n",
    "    StructField(\"update_timestamp\", TimestampType()),\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"price\", FloatType()),\n",
    "    StructField(\"quantity\", LongType()),\n",
    "    StructField(\"status\", StringType()),\n",
    "    StructField(\"stock_symbol\", StringType()),\n",
    "    StructField(\"trader_id\", StringType()),\n",
    "    StructField(\"type\", StringType()),\n",
    "    StructField(\"sub_transactions\", ArrayType(StructType([\n",
    "        StructField(\"sub_transaction_id\", StringType()),\n",
    "        StructField(\"price_executed\", FloatType()),\n",
    "        StructField(\"quantity_fulfilled\", LongType()),\n",
    "        StructField(\"matched_order_id\", StringType()),\n",
    "        StructField(\"timestamp\", TimestampType())\n",
    "    ]))),\n",
    "    StructField(\"_metadata\", MapType(StringType(), StringType()))\n",
    "])\n",
    "\n",
    "# Retrieve last execution timestamp\n",
    "def get_last_execution_time():\n",
    "    try:\n",
    "        tracking_df = spark.sql(f\"SELECT max(last_execution_time) FROM {tracking_table_name}\")\n",
    "        last_execution_time = tracking_df.collect()[0][0]\n",
    "        return last_execution_time if last_execution_time else \"1970-01-01T00:00:00\"\n",
    "    except:\n",
    "        return \"1970-01-01T00:00:00\"\n",
    "\n",
    "# Save execution timestamp\n",
    "def save_execution_time(timestamp):\n",
    "    timestamp_df = spark.createDataFrame([(timestamp,)], [\"last_execution_time\"])\n",
    "    timestamp_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(tracking_table_name)\n",
    "\n",
    "# Get timestamps\n",
    "last_execution_time = get_last_execution_time()\n",
    "current_execution_time = datetime.utcnow().isoformat()\n",
    "\n",
    "# Read data from Elasticsearch\n",
    "es_index_pattern = \"trade_txn_*\"\n",
    "es_df = spark.read.format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.nodes\", es_cloud_url) \\\n",
    "    .option(\"es.port\", \"443\") \\\n",
    "    .option(\"es.net.ssl\", \"true\") \\\n",
    "    .option(\"es.nodes.wan.only\", \"true\") \\\n",
    "    .option(\"es.nodes.discovery\", \"false\") \\\n",
    "    .option(\"es.net.http.auth.user\", es_user) \\\n",
    "    .option(\"es.net.http.auth.pass\", es_pass) \\\n",
    "    .option(\"es.read.metadata\", \"true\") \\\n",
    "    .option(\"es.resource\", es_index_pattern) \\\n",
    "    .option(\"es.query\", f'{{\"range\": {{\"update_timestamp\": {{\"gt\": \"{last_execution_time}\"}}}}}}') \\\n",
    "    .schema(trade_txn_schema) \\\n",
    "    .load()\n",
    "\n",
    "# Extract _index from _metadata\n",
    "es_df = es_df.withColumn(\"delta_table\", col(\"_metadata\").getItem(\"_index\"))\n",
    "\n",
    "# Process data if new records exist\n",
    "if es_df.count() > 0:\n",
    "    for index_name in es_df.select(\"delta_table\").distinct().collect():\n",
    "        table_name = index_name[\"delta_table\"]\n",
    "        table_hive_name = f\"hive_metastore.default.{table_name}\"\n",
    "        \n",
    "        # Filter records for this specific table\n",
    "        table_df = es_df.filter(trim(col(\"delta_table\")) == lit(table_name.strip())).drop(\"_metadata\")\n",
    "        \n",
    "        # Check if Delta table exists, if not create it\n",
    "        try:\n",
    "            spark.sql(f\"DESCRIBE TABLE {table_hive_name}\")\n",
    "        except:\n",
    "            table_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_hive_name)\n",
    "        \n",
    "        # Perform upsert using merge\n",
    "        delta_table = DeltaTable.forName(spark, table_hive_name)\n",
    "        (delta_table.alias(\"tgt\")\n",
    "            .merge(table_df.alias(\"src\"), \"tgt.order_id = src.order_id\")\n",
    "            .whenMatchedUpdateAll()\n",
    "            .whenNotMatchedInsertAll()\n",
    "            .execute())\n",
    "    \n",
    "    # Save new execution timestamp\n",
    "    current_execution_time = es_df.agg(max(\"update_timestamp\")).collect()[0][0].isoformat() # to be removed when executing continuously\n",
    "    save_execution_time(current_execution_time)\n",
    "\n",
    "print(\"Job completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-04-03 20:36:05",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
